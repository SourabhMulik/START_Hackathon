{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.features import geometry_mask\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'client_secret.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moauth2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m credentials\n\u001b[1;32m      8\u001b[0m Credentials_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclient_secret.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m creds \u001b[38;5;241m=\u001b[39m \u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_authorized_user_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCredentials_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 创建 Google Drive API 客户端\u001b[39;00m\n\u001b[1;32m     12\u001b[0m service \u001b[38;5;241m=\u001b[39m build(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrive\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv3\u001b[39m\u001b[38;5;124m'\u001b[39m, http\u001b[38;5;241m=\u001b[39mAuthorizedHttp(creds, http\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/google/oauth2/credentials.py:448\u001b[0m, in \u001b[0;36mCredentials.from_authorized_user_file\u001b[0;34m(cls, filename, scopes)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_authorized_user_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename, scopes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;124;03m\"\"\"Creates a Credentials instance from an authorized user json file.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m        ValueError: If the file is not in the expected format.\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m    449\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_authorized_user_info(data, scopes)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'client_secret.json'"
     ]
    }
   ],
   "source": [
    "# pip install pydrive\n",
    "# pip install google-api-python-client google-auth-httplib2\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_httplib2 import AuthorizedHttp\n",
    "from google.oauth2 import credentials\n",
    "\n",
    "Credentials_file = 'client_secret.json'\n",
    "creds = credentials.Credentials.from_authorized_user_file(Credentials_file)\n",
    "\n",
    "# 创建 Google Drive API 客户端\n",
    "service = build('drive', 'v3', http=AuthorizedHttp(creds, http=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Reprojection\n",
    "\n",
    "**CRS of Land as reprojection reference.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define years list\n",
    "years = range(2000, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "data_path = 'Datasets_Hackathon'\n",
    "\n",
    "reproject_path = 'Datasets_Hackathon/Reprojected_Data'\n",
    "if not os.path.exists(reproject_path):\n",
    "        os.makedirs(reproject_path)\n",
    "        \n",
    "csv_path = 'For_dashboard'\n",
    "if not os.path.exists(csv_path):\n",
    "        os.makedirs(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dynamic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reprojecting all dynamic data\n",
    "\"\"\"\n",
    "\n",
    "# List of data categories with their appropriate resampling methods and filename formats\n",
    "datasets = [\n",
    "    {'short_name':'land', 'name': 'Land_Cover_Data', 'file_format': '{year}LCT.tif', 'is_reference': True},\n",
    "    {'short_name':'rainfall', 'name': 'Climate_Precipitation_Data', 'file_format': '{year}R.tif', 'resampling': Resampling.bilinear},\n",
    "    {'short_name':'pop', 'name': 'Gridded_Population_Density_Data', 'file_format': 'Assaba_Pop_{year}.tif','resampling': Resampling.bilinear},\n",
    "    {'short_name':'popdens', 'name': 'Gridded_Population_Density_Data', 'file_format': 'mrt_pd_{year}_1km.tif','resampling': Resampling.bilinear},\n",
    "    {'short_name':'gpp', 'name': 'Gross_Primary_Production_GPP', 'file_format': '{year}_GP.tif','resampling': Resampling.nearest},\n",
    "]\n",
    "\n",
    "# Dictionary to store data for all years and all datasets\n",
    "all_data = {d['short_name']: {} for d in datasets}\n",
    "\n",
    "# Loop through each year\n",
    "for year in years:\n",
    "    print(f\"Processing data for year {year}...\")\n",
    "    \n",
    "    # First, open the reference dataset (GPP)\n",
    "    ref_dataset = next(d for d in datasets if d['is_reference'])\n",
    "    ref_file = os.path.join(data_path, ref_dataset['name'], ref_dataset['file_format'].format(year=year))\n",
    "    \n",
    "    try:\n",
    "        with rasterio.open(ref_file) as src_ref:\n",
    "            # Get reference metadata\n",
    "            dst_crs = src_ref.crs\n",
    "            dst_transform = src_ref.transform\n",
    "            dst_height = src_ref.height\n",
    "            dst_width = src_ref.width\n",
    "            \n",
    "            # Read reference data (land)\n",
    "            land_data = src_ref.read(1)\n",
    "            all_data['land'][year] = land_data\n",
    "            \n",
    "            # Store reference profile for output files\n",
    "            profile = src_ref.profile.copy()\n",
    "            profile.update(dtype=rasterio.float64, count=1)\n",
    "            \n",
    "            # Process each non-reference dataset\n",
    "            for dataset in [d for d in datasets if not d.get('is_reference', False)]:\n",
    "                dataset_name = dataset['short_name']  # Extract short name\n",
    "                \n",
    "                # Construct input filename using the file format template\n",
    "                input_file = os.path.join(data_path, dataset['name'], dataset['file_format'].format(year=year))\n",
    "                output_file = os.path.join(reproject_path, f\"{dataset_name}_reprojected_{year}.tif\")\n",
    "                \n",
    "                # Create destination array\n",
    "                dst_array = np.zeros((dst_height, dst_width), dtype=rasterio.float32)\n",
    "                \n",
    "                # Open and reproject\n",
    "                try:\n",
    "                    with rasterio.open(input_file) as src:\n",
    "                        reproject(\n",
    "                            source=rasterio.band(src, 1),\n",
    "                            destination=dst_array,\n",
    "                            src_transform=src.transform,\n",
    "                            src_crs=src.crs,\n",
    "                            dst_transform=dst_transform,\n",
    "                            dst_crs=dst_crs,\n",
    "                            resampling=dataset['resampling']\n",
    "                        )\n",
    "                        \n",
    "                        # Store in all_data dictionary by year\n",
    "                        all_data[dataset_name][year] = dst_array\n",
    "                        \n",
    "                        # Save reprojected data\n",
    "                        with rasterio.open(output_file, 'w', **profile) as dst:\n",
    "                            dst.write(dst_array, 1)\n",
    "                            \n",
    "                        print(f\"Successfully reprojected and saved {output_file}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {input_file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing year {year}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Static Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cols(gdf):\n",
    "    \"\"\"\n",
    "    Preprocesses GeoDataFrame for shapefile compatibility.\n",
    "\n",
    "    Converts large integers to strings and formats datetime columns as ISO strings.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf: geopandas.GeoDataFrame\n",
    "        Input GeoDataFrame.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    gdf_copy: geopandas.GeoDataFrame\n",
    "        Processed GeoDataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    gdf_copy = gdf.copy()\n",
    "    \n",
    "    for col in gdf_copy.columns:\n",
    "        # Convert large integers to strings, eg. \"osm_id\" col in Water shp\n",
    "        if gdf_copy[col].dtype == 'float64':\n",
    "            gdf_copy[col] = gdf_copy[col].astype(str)\n",
    "        \n",
    "        # Handle datetime columns, eg. \"date\" cols\n",
    "        if pd.api.types.is_datetime64_any_dtype(gdf_copy[col]): \n",
    "            # Convert to string in ISO format\n",
    "            gdf_copy[col] = gdf_copy[col].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return gdf_copy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_and_save_shapefile(input_gdf, dst_crs, output_path):\n",
    "    \"\"\"\n",
    "    Reprojects GeoDataFrame and saves as shapefile.\n",
    "\n",
    "    Handles large integers, datetime, and CRS conversion.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_gdf: geopandas.GeoDataFrame\n",
    "        Input GeoDataFrame.\n",
    "        \n",
    "    dst_crs: str or pyproj.CRS\n",
    "        Destination CRS.\n",
    "\n",
    "    output_path: str\n",
    "        Output shapefile path.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    gdf_reprojected: geopandas.GeoDataFrame\n",
    "        Reprojected GeoDataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    gdf_preprocessed = preprocess_cols(input_gdf)\n",
    "    \n",
    "    # Reproject\n",
    "    gdf_reprojected = gdf_preprocessed.to_crs(dst_crs)\n",
    "    \n",
    "    # Save with modified field handling\n",
    "    gdf_reprojected.to_file(output_path)\n",
    "    \n",
    "    return gdf_reprojected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shp_to_tif(gdf, ref_raster, output_tif_path):\n",
    "    \"\"\"\n",
    "    Converts GeoDataFrame to TIFF raster file based on a reference raster.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf: geopandas.GeoDataFrame\n",
    "        Input GeoDataFrame.\n",
    "        \n",
    "    ref_raster: rasterio.DatasetReader\n",
    "        Reference raster dataset to use for getting transform, crs, width, height.\n",
    "    \n",
    "    output_tif_path: str\n",
    "        Output path for the TIFF raster file.\n",
    "    \"\"\"\n",
    "    shapes = ((geom, 1) for geom in gdf.geometry)\n",
    "    \n",
    "    # Use rasterio.features.rasterize to rasterize the shapefile\n",
    "    rasterized_array = rasterio.features.rasterize(\n",
    "        shapes=shapes,\n",
    "        out_shape=(ref_raster.height, ref_raster.width),\n",
    "        transform=ref_raster.transform,\n",
    "        fill=ref_raster.nodata,\n",
    "        dtype='float64'\n",
    "    )\n",
    "    \n",
    "    # Create a profile for the new TIFF file\n",
    "    profile = ref_raster.profile.copy()\n",
    "    profile.update({\n",
    "        'dtype': 'float64',\n",
    "        'compress': 'lzw'\n",
    "    })\n",
    "    \n",
    "    # Write the rasterized shapefile to a new TIFF file\n",
    "    with rasterio.open(output_tif_path, 'w', **profile) as dst:\n",
    "        dst.write(rasterized_array, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "Road_path = \"Datasets_Hackathon/Streamwater_Line_Road_Network/Main_Road.shp\"\n",
    "Water_path = \"Datasets_Hackathon/Streamwater_Line_Road_Network/Streamwater.shp\"\n",
    "Dist_path = 'Datasets_Hackathon/Admin_layers/Assaba_Districts_layer.shp'\n",
    "ref_path = 'Datasets_Hackathon/Land_Cover_Data/2010LCT.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reproject static data\n",
    "\"\"\"\n",
    "\n",
    "# Load shapefiles\n",
    "road = gpd.read_file(Road_path)\n",
    "water = gpd.read_file(Water_path)\n",
    "dist = gpd.read_file(Dist_path)\n",
    "\n",
    "# Open reference raster to get CRS and other parameters\n",
    "with rasterio.open(ref_path) as src_ref:\n",
    "    dst_crs = src_ref.crs\n",
    "    dst_transform = src_ref.transform\n",
    "    dst_height = src_ref.height\n",
    "    dst_width = src_ref.width\n",
    "    nodata_value = src_ref.nodata\n",
    "    \n",
    "    # Read the reference raster data\n",
    "    ref_array = src_ref.read(1)\n",
    "    \n",
    "    # Create a mask for valid data (where values are not nodata)\n",
    "    valid_mask = ref_array != nodata_value\n",
    "\n",
    "    # Reproject and save shapefiles with field handling\n",
    "    road_reprojected = reproject_and_save_shapefile(\n",
    "        road, \n",
    "        dst_crs, \n",
    "        os.path.join(reproject_path, 'road_reprojected.shp')\n",
    "    )\n",
    "    \n",
    "    water_reprojected = reproject_and_save_shapefile(\n",
    "        water, \n",
    "        dst_crs, \n",
    "        os.path.join(reproject_path, 'water_reprojected.shp')\n",
    "    )\n",
    "    \n",
    "    dist_reprojected = reproject_and_save_shapefile(\n",
    "        dist, \n",
    "        dst_crs, \n",
    "        os.path.join(reproject_path, 'dist_reprojected.shp')\n",
    "    )\n",
    "\n",
    "    # Save each reprojected shapefile as TIFF\n",
    "    shp_to_tif(road_reprojected, src_ref, os.path.join(reproject_path, 'road_reprojected.tif'))\n",
    "    shp_to_tif(water_reprojected, src_ref, os.path.join(reproject_path, 'water_reprojected.tif'))\n",
    "    shp_to_tif(dist_reprojected, src_ref, os.path.join(reproject_path, 'dist_reprojected.tif'))\n",
    "\n",
    "    # Create a new raster with the valid data mask\n",
    "    profile = src_ref.profile.copy()\n",
    "    profile.update({\n",
    "        'dtype': rasterio.float64,\n",
    "        'nodata': nodata_value,\n",
    "        'compress': 'lzw'\n",
    "    })\n",
    "\n",
    "    # Mask the reference raster\n",
    "    masked_array = np.where(valid_mask, ref_array, nodata_value).astype(rasterio.float32)\n",
    "\n",
    "    # Save the masked raster\n",
    "    masked_raster_path = os.path.join(reproject_path, '2010LCT_masked.tif')\n",
    "    with rasterio.open(masked_raster_path, 'w', **profile) as dst:\n",
    "        dst.write(masked_array, 1)\n",
    "\n",
    "print(\"Reprojection and masking completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Overview of invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview of all data\n",
    "all_data\n",
    "\n",
    "# invalid data in each dict:\n",
    "# 1. negative value:\n",
    "#   -128.0 in land\n",
    "#   -3.4028235e+38 in rainfall and pop\n",
    "# 2. positive value:\n",
    "#   65535 in GPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Functions for masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(data_dict, invalid_criteria=None):\n",
    "    \"\"\"\n",
    "    Creates masks for data based on specified invalid criteria.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dict: dict\n",
    "        Dictionary with data types as keys and yearly data arrays as values.\n",
    "    invalid_criteria: dict, optional\n",
    "        Dictionary defining invalid conditions per data type using 'condition' lambda functions.\n",
    "        Defaults to masking values less than 0 if not provided.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    mask_dict: dict\n",
    "        Dictionary with boolean masks where True marks invalid data.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Custom invalid criteria can be defined for each data type.\n",
    "    - Default condition is values less than 0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default invalid data criteria\n",
    "    default_criteria = {\n",
    "        'default': {'condition': lambda x: (x < 0)}\n",
    "    }\n",
    "    \n",
    "    # Merge default criteria with provided criteria\n",
    "    if invalid_criteria is None:\n",
    "        invalid_criteria = default_criteria\n",
    "    else:\n",
    "        for key, value in default_criteria.items():\n",
    "            if key not in invalid_criteria:\n",
    "                invalid_criteria[key] = value\n",
    "    \n",
    "    # Create mask dictionary\n",
    "    mask_dict = {}\n",
    "    \n",
    "    # Iterate through data types\n",
    "    for data_type, year_data in data_dict.items():\n",
    "        mask_dict[data_type] = {}\n",
    "        \n",
    "        # Determine criteria for this data type\n",
    "        criteria = invalid_criteria.get(data_type, invalid_criteria['default'])\n",
    "        condition = criteria['condition']\n",
    "        \n",
    "        # Create masks for each year\n",
    "        for year, array in year_data.items():\n",
    "            # Apply the condition to create a boolean mask\n",
    "            mask_dict[data_type][year] = condition(array)\n",
    "    \n",
    "    return mask_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(data_dict, mask_dict):\n",
    "    \"\"\"\n",
    "    Applies masks to a dictionary of data arrays.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dict: dict\n",
    "        Dictionary with data arrays by data type and year.\n",
    "    mask_dict: dict\n",
    "        Dictionary with boolean masks by data type and year, where True indicates invalid data.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    masked_data_dict: dict\n",
    "        Dictionary with masked data arrays, where invalid values are masked.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function uses `numpy.ma.array()` to create masked arrays.\n",
    "    - The mask is applied to the data arrays, so invalid data points \n",
    "        (those corresponding to `True` in the mask) will be masked (ignored).\n",
    "    \"\"\"\n",
    "\n",
    "    masked_data_dict = {}\n",
    "    \n",
    "    # Iterate through data types\n",
    "    for data_type, year_data in data_dict.items():\n",
    "        masked_data_dict[data_type] = {}\n",
    "        \n",
    "        # Apply mask for each year\n",
    "        for year, array in year_data.items():\n",
    "            masked_data_dict[data_type][year] = np.ma.array(\n",
    "                array, \n",
    "                mask=mask_dict[data_type][year]\n",
    "            )\n",
    "    \n",
    "    return masked_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_df(masked_data_dict):    \n",
    "    \"\"\"\n",
    "    Convert masked data dictionary to a comprehensive DataFrame with location information.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    masked_data_dict : dict\n",
    "        Nested dictionary of masked arrays\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Comprehensive DataFrame with data information and coordinates\n",
    "    \"\"\"\n",
    "    \n",
    "    data_rows = []\n",
    "    \n",
    "    # Iterate through data types\n",
    "    for data_type, year_data in masked_data_dict.items():\n",
    "        # Iterate through years\n",
    "        for year, masked_array in year_data.items():\n",
    "            # Create a grid of row and column indices\n",
    "            rows, cols = np.indices(masked_array.shape)\n",
    "            \n",
    "            # Create a mask for non-masked elements\n",
    "            valid_mask = ~masked_array.mask\n",
    "            \n",
    "            # Get valid data points\n",
    "            valid_data = masked_array.data[valid_mask]\n",
    "            valid_rows = rows[valid_mask]\n",
    "            valid_cols = cols[valid_mask]\n",
    "            \n",
    "            # Create rows for each valid data point\n",
    "            data_rows.extend([{\n",
    "                'year': year,\n",
    "                'lat': valid_rows[idx],\n",
    "                'lon': valid_cols[idx],\n",
    "                data_type: value\n",
    "            } for idx, value in enumerate(valid_data)])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_rows)\n",
    "    \n",
    "    # If multiple data types exist, pivot and merge\n",
    "    if len(df.columns) > 4:\n",
    "        # Pivot the DataFrame to have one row per (year, lat, lon)\n",
    "        df_pivoted = df.pivot_table(\n",
    "            index=['year', 'lat', 'lon'], \n",
    "            values=df.columns[3:],  # Use the data type columns\n",
    "            aggfunc='first'         # Use 'first' to aggregate if needed\n",
    "        ).reset_index()\n",
    "        \n",
    "        return df_pivoted\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_masked_data(masked_data_dict):\n",
    "    \"\"\"\n",
    "    Computes summary statistics for masked data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    masked_data_dict: dict\n",
    "        Dictionary of masked arrays where keys represent data types and years.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    dict\n",
    "        A dictionary with summary statistics (e.g., valid points, masked percentage, \n",
    "        min, max, mean, and median) for each data type and year.\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_stats = {}\n",
    "    \n",
    "    # Iterate through data types\n",
    "    for data_type, year_data in masked_data_dict.items():\n",
    "        summary_stats[data_type] = {}\n",
    "        \n",
    "        # Compute statistics for each year\n",
    "        for year, masked_array in year_data.items():\n",
    "            # Compute statistics on valid (unmasked) data\n",
    "            valid_data = masked_array.compressed()\n",
    "            \n",
    "            summary_stats[data_type][year] = {\n",
    "                'total_points': masked_array.size,\n",
    "                'valid_points': len(valid_data),\n",
    "                'masked_points': masked_array.size - len(valid_data),\n",
    "                'masked_percentage': (masked_array.size - len(valid_data)) / masked_array.size * 100,\n",
    "                'min': np.min(valid_data) if len(valid_data) > 0 else None,\n",
    "                'max': np.max(valid_data) if len(valid_data) > 0 else None,\n",
    "                'mean': np.mean(valid_data) if len(valid_data) > 0 else None,\n",
    "                'median': np.median(valid_data) if len(valid_data) > 0 else None\n",
    "            }\n",
    "    \n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_dict, invalid_criteria=None):\n",
    "    \"\"\"\n",
    "    Main processing function for data masking.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dict : dict\n",
    "        Nested dictionary of data arrays\n",
    "    invalid_criteria : dict, optional\n",
    "        Custom invalid data criteria\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Processed data results\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> result = main(data_dict, invalid_criteria=lambda x: x < 0)\n",
    "    >>> result['dataframe']  # Access the resulting DataFrame\n",
    "    >>> result['summary']  # View the summary statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create masks\n",
    "    mask_dict = create_mask(data_dict, invalid_criteria)\n",
    "    \n",
    "    # Apply masks\n",
    "    masked_data_dict = apply_mask(data_dict, mask_dict)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    # df = dict_to_df(masked_data_dict)\n",
    "    df = dict_to_df(masked_data_dict)\n",
    "\n",
    "    # Get summary statistics\n",
    "    summary = analyze_masked_data(masked_data_dict)\n",
    "    \n",
    "    return {\n",
    "        'masked_data': masked_data_dict,\n",
    "        'dataframe': df,\n",
    "        'summary': summary,\n",
    "        'mask_dict': mask_dict\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"Nested Dictionary Data Masking Module\")\n",
    "    print(\"Supports NumPy array-compatible masking for different data types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Processing - dynamic data - applying mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom criteria\n",
    "\n",
    "custom_criteria_dynamic = {\n",
    "    'land': {'condition': lambda x: np.logical_or(x < 0, x == 255)},\n",
    "    'gpp': {'condition': lambda x: np.logical_or(x == 65533, x == 65535)},\n",
    "    'pop': {'condition': lambda x: x < 0}\n",
    "}\n",
    "\n",
    "# Process data\n",
    "prepared_dict = main(all_data, invalid_criteria=custom_criteria_dynamic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check masking results\n",
    "data_types = ['land', 'gpp', 'pop', 'popdens', 'rainfall']\n",
    "show_years = [2023, 2023, 2020, 2020, 2023]\n",
    "\n",
    "for data_type, year in zip(data_types, show_years):\n",
    "    print(f\"Masking Summary for {data_type.capitalize()} in {year}\\n {prepared_dict['summary'][data_type][year]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df = prepared_dict['dataframe']\n",
    "prepared_df[prepared_df['gpp']>100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Processing - static data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data = {}\n",
    "\n",
    "for i in ['road', 'water', 'dist']:\n",
    "    static_year_data = {}\n",
    "\n",
    "    with rasterio.open(os.path.join(reproject_path, f'{i}_reprojected.tif')) as raster:\n",
    "        raster_arr = raster.read(1)\n",
    "        for year in years:\n",
    "            static_year_data[year] = raster_arr\n",
    "        \n",
    "    static_data[i] = static_year_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invalid criteria for static data\n",
    "custom_criteria_static = {\n",
    "    'road': {'condition': lambda x: x == -128},\n",
    "    'water': {'condition': lambda x: x == -128},\n",
    "    'dist': {'condition': lambda x: x == -128}\n",
    "}\n",
    "\n",
    "# Process data\n",
    "prepared_dict_2 = main(static_data, invalid_criteria=custom_criteria_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check masking results\n",
    "data_types_2 = ['road', 'water', 'dist']\n",
    "years_2 = [2023, 2023, 2023]\n",
    "\n",
    "for data_type, year in zip(data_types_2, years_2):\n",
    "    print(f\"Masking Summary for {data_type.capitalize()} in {year}\\n {prepared_dict_2['summary'][data_type][year]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = prepared_dict_2['dataframe']\n",
    "static_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prepared_df = pd.merge(prepared_df, static_df, on=['year','lat', 'lon'], how='left')\n",
    "second_prepared_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Finalise dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove invalid data;\n",
    "- Combine dynamic and static data;\n",
    "- Add actual latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop invalid data\n",
    "clean_df = second_prepared_df.dropna(subset=['land'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value in dynamic data like 'pop' and static data like 'road', 'water' is not invalid \n",
    "# but states that there is 0 people/no road/no water etc.\n",
    "# so fill with 0\n",
    "clean_df = clean_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import import_ipynb\n",
    "import import_ipynb\n",
    "from Actual_Coords_Projection import gen_coords\n",
    "\n",
    "lat_center = 16.759763\n",
    "lon_center = -11.725705\n",
    "pixel_resolution = 463.312716525\n",
    "rows, cols = 769, 565\n",
    "\n",
    "coords_dict = gen_coords(lat_center, lon_center, pixel_resolution, rows, cols)\n",
    "coords_dict\n",
    "\n",
    "clean_df['actual_lat'] = clean_df.apply(lambda row: coords_dict.get((row['lat'], row['lon']), (None, None))[0], axis=1)\n",
    "clean_df['actual_lon'] = clean_df.apply(lambda row: coords_dict.get((row['lat'], row['lon']), (None, None))[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[clean_df.select_dtypes(include=['float64']).columns] = clean_df.select_dtypes(include=['float64']).astype('float32')\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Clean Data info\\n:{clean_df.info()}\")\n",
    "print(f\"Clean Data Null value summary\\n:{clean_df.isnull().sum()}\")\n",
    "print(f\"Clean Data NaN value summary\\n:{clean_df.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pydrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file to local (git res.)\n",
    "# clean_df.to_json(os.path.join(csv_path,f\"clean_data.json\"))\n",
    "clean_df.to_csv(os.path.join(csv_path,f\"clean_data.csv\"), index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local file For_dashboard/clean_data.csv has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# 删除本地的CSV文件\n",
    "if os.path.exists(os.path.join(csv_path, f\"clean_data.csv\")):\n",
    "    os.remove(os.path.join(csv_path, f\"clean_data.csv\"))\n",
    "    print(f'Local file {os.path.join(csv_path, f\"clean_data.csv\")} has been deleted.')\n",
    "else:\n",
    "    print(f'File {os.path.join(csv_path, f\"clean_data.csv\")} does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义文件路径\n",
    "file_path = os.path.join(csv_path, f\"clean_data.csv\")\n",
    "folder_id = 'START_Hackathon'\n",
    "\n",
    "# 上传文件到指定文件夹\n",
    "media = MediaFileUpload(CSV_FILE, mimetype='text/csv')\n",
    "if FOLDER_ID:\n",
    "    file_metadata['parents'] = [FOLDER_ID]\n",
    "file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "print(f\"File ID: {file.get('id')}\")\n",
    "\n",
    "# 删除本地的CSV文件\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(f'Local file {file_path} has been deleted.')\n",
    "else:\n",
    "    print(f'File {file_path} does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df.to_csv(os.path.join(csv_path, f\"clean_df.csv\"), index=0)\n",
    "# with open(os.path.join(csv_path, f\"clean_df.csv\"), mode='r', encoding='utf-8') as csv_file:\n",
    "#     csv_reader = csv.DictReader(csv_file) \n",
    "#     rows = list(csv_reader)\n",
    "\n",
    "# # save csv to json\n",
    "# with open(os.path.join(csv_path, f\"clean_data.json\"), mode='w', encoding='utf-8') as json_file:\n",
    "#     json.dump(rows, json_file, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
