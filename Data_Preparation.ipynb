{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reprojection\n",
    "\n",
    "**CRS of Land as reprojection reference.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dynamic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define years list\n",
    "years = range(2000, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "data_path = 'Datasets_Hackathon'\n",
    "\n",
    "reproject_path = 'Datasets_Hackathon/Reprojected_Data'\n",
    "if not os.path.exists(reproject_path):\n",
    "        os.makedirs(reproject_path)\n",
    "        \n",
    "csv_path = 'For_dashboard'\n",
    "if not os.path.exists(csv_path):\n",
    "        os.makedirs(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for year 2000...\n",
      "Error processing year 2000: Datasets_Hackathon/Land_Cover_Data/2000LCT.tif: No such file or directory\n",
      "Processing data for year 2001...\n",
      "Error processing year 2001: Datasets_Hackathon/Land_Cover_Data/2001LCT.tif: No such file or directory\n",
      "Processing data for year 2002...\n",
      "Error processing year 2002: Datasets_Hackathon/Land_Cover_Data/2002LCT.tif: No such file or directory\n",
      "Processing data for year 2003...\n",
      "Error processing year 2003: Datasets_Hackathon/Land_Cover_Data/2003LCT.tif: No such file or directory\n",
      "Processing data for year 2004...\n",
      "Error processing year 2004: Datasets_Hackathon/Land_Cover_Data/2004LCT.tif: No such file or directory\n",
      "Processing data for year 2005...\n",
      "Error processing year 2005: Datasets_Hackathon/Land_Cover_Data/2005LCT.tif: No such file or directory\n",
      "Processing data for year 2006...\n",
      "Error processing year 2006: Datasets_Hackathon/Land_Cover_Data/2006LCT.tif: No such file or directory\n",
      "Processing data for year 2007...\n",
      "Error processing year 2007: Datasets_Hackathon/Land_Cover_Data/2007LCT.tif: No such file or directory\n",
      "Processing data for year 2008...\n",
      "Error processing year 2008: Datasets_Hackathon/Land_Cover_Data/2008LCT.tif: No such file or directory\n",
      "Processing data for year 2009...\n",
      "Error processing year 2009: Datasets_Hackathon/Land_Cover_Data/2009LCT.tif: No such file or directory\n",
      "Processing data for year 2010...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2010.tif\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/pop_reprojected_2010.tif\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/popdens_reprojected_2010.tif\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2010.tif\n",
      "Processing data for year 2011...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2011.tif\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2011.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2011.tif: No such file or directory\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2011_1km.tif: Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2011_1km.tif: No such file or directory\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2011.tif\n",
      "Processing data for year 2012...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2012.tif\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2012.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2012.tif: No such file or directory\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2012_1km.tif: Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2012_1km.tif: No such file or directory\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2012.tif\n",
      "Processing data for year 2013...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2013.tif\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2013.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2013.tif: No such file or directory\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2013_1km.tif: Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2013_1km.tif: No such file or directory\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2013.tif\n",
      "Processing data for year 2014...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2014.tif\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2014.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2014.tif: No such file or directory\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2014_1km.tif: Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2014_1km.tif: No such file or directory\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2014.tif\n",
      "Processing data for year 2015...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2015.tif\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/pop_reprojected_2015.tif\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/popdens_reprojected_2015.tif\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2015.tif\n",
      "Processing data for year 2016...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2016.tif\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2016.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2016.tif: No such file or directory\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2016_1km.tif: Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2016_1km.tif: No such file or directory\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2016.tif\n",
      "Processing data for year 2017...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2017.tif\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2017.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2017.tif: No such file or directory\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2017_1km.tif: Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2017_1km.tif: No such file or directory\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2017.tif\n",
      "Processing data for year 2018...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2018.tif\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2018.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2018.tif: No such file or directory\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2018_1km.tif: Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2018_1km.tif: No such file or directory\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2018.tif\n",
      "Processing data for year 2019...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2019.tif\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2019.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2019.tif: No such file or directory\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2019_1km.tif: Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2019_1km.tif: No such file or directory\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2019.tif\n",
      "Processing data for year 2020...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2020.tif\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/pop_reprojected_2020.tif\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/popdens_reprojected_2020.tif\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2020.tif\n",
      "Processing data for year 2021...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2021.tif\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2021.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2021.tif: No such file or directory\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2021_1km.tif: Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2021_1km.tif: No such file or directory\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2021.tif\n",
      "Processing data for year 2022...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2022.tif\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2022.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2022.tif: No such file or directory\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2022_1km.tif: Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2022_1km.tif: No such file or directory\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2022.tif\n",
      "Processing data for year 2023...\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/rainfall_reprojected_2023.tif\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2023.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2023.tif: No such file or directory\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2023_1km.tif: Datasets_Hackathon/Gridded_Population_Density_Data/mrt_pd_2023_1km.tif: No such file or directory\n",
      "Successfully reprojected and saved Datasets_Hackathon/Reprojected_Data/gpp_reprojected_2023.tif\n"
     ]
    }
   ],
   "source": [
    "'''Reproject tif data'''\n",
    "\n",
    "# List of data categories with their appropriate resampling methods and filename formats\n",
    "datasets = [\n",
    "    {'short_name':'land', 'name': 'Land_Cover_Data', 'file_format': '{year}LCT.tif', 'is_reference': True},\n",
    "    {'short_name':'rainfall', 'name': 'Climate_Precipitation_Data', 'file_format': '{year}R.tif', 'resampling': Resampling.bilinear},\n",
    "    {'short_name':'pop', 'name': 'Gridded_Population_Density_Data', 'file_format': 'Assaba_Pop_{year}.tif','resampling': Resampling.bilinear},\n",
    "    {'short_name':'popdens', 'name': 'Gridded_Population_Density_Data', 'file_format': 'mrt_pd_{year}_1km.tif','resampling': Resampling.bilinear},\n",
    "    {'short_name':'gpp', 'name': 'Gross_Primary_Production_GPP', 'file_format': '{year}_GP.tif','resampling': Resampling.nearest},\n",
    "]\n",
    "\n",
    "# Dictionary to store data for all years and all datasets\n",
    "all_data = {}\n",
    "for dataset in datasets:\n",
    "    short_name = dataset['short_name']\n",
    "    all_data[short_name] = {}\n",
    "\n",
    "# Loop through each year\n",
    "for year in years:\n",
    "    print(f\"Processing data for year {year}...\")\n",
    "    \n",
    "    # First, open the reference dataset (GPP)\n",
    "    ref_dataset = next(d for d in datasets if d['is_reference'])\n",
    "    ref_file = os.path.join(data_path, ref_dataset['name'], ref_dataset['file_format'].format(year=year))\n",
    "    \n",
    "    try:\n",
    "        with rasterio.open(ref_file) as src_ref:\n",
    "            # Get reference metadata\n",
    "            dst_crs = src_ref.crs\n",
    "            dst_transform = src_ref.transform\n",
    "            dst_height = src_ref.height\n",
    "            dst_width = src_ref.width\n",
    "            \n",
    "            # Read reference data (land)\n",
    "            land_data = src_ref.read(1)\n",
    "            all_data['land'][year] = land_data\n",
    "            \n",
    "            # Store reference profile for output files\n",
    "            profile = src_ref.profile.copy()\n",
    "            profile.update(dtype=rasterio.float32, count=1)\n",
    "            \n",
    "            # Process each non-reference dataset\n",
    "            for dataset in [d for d in datasets if not d.get('is_reference', False)]:\n",
    "                dataset_name = dataset['short_name']  # Extract short name\n",
    "                \n",
    "                # Construct input filename using the file format template\n",
    "                input_file = os.path.join(data_path, dataset['name'], dataset['file_format'].format(year=year))\n",
    "                output_file = os.path.join(reproject_path, f\"{dataset_name}_reprojected_{year}.tif\")\n",
    "                \n",
    "                # Create destination array\n",
    "                dst_array = np.zeros((dst_height, dst_width), dtype=rasterio.float32)\n",
    "                \n",
    "                # Open and reproject\n",
    "                try:\n",
    "                    with rasterio.open(input_file) as src:\n",
    "                        reproject(\n",
    "                            source=rasterio.band(src, 1),\n",
    "                            destination=dst_array,\n",
    "                            src_transform=src.transform,\n",
    "                            src_crs=src.crs,\n",
    "                            dst_transform=dst_transform,\n",
    "                            dst_crs=dst_crs,\n",
    "                            resampling=dataset['resampling']\n",
    "                        )\n",
    "                        \n",
    "                        # Store in all_data dictionary by year\n",
    "                        all_data[dataset_name][year] = dst_array\n",
    "                        \n",
    "                        # Save reprojected data\n",
    "                        with rasterio.open(output_file, 'w', **profile) as dst:\n",
    "                            dst.write(dst_array, 1)\n",
    "                            \n",
    "                        print(f\"Successfully reprojected and saved {output_file}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {input_file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing year {year}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Static Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "Road_path = \"Datasets_Hackathon/Streamwater_Line_Road_Network/Main_Road.shp\"\n",
    "Water_path = \"Datasets_Hackathon/Streamwater_Line_Road_Network/Streamwater.shp\"\n",
    "Dist_path = 'Datasets_Hackathon/Admin_layers/Assaba_Districts_layer.shp'\n",
    "ref_path = 'Datasets_Hackathon/Land_Cover_Data/2010LCT.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cols(gdf):\n",
    "    \"\"\"\n",
    "    Preprocess datetime columns to ensure compatibility with shapefile writing\n",
    "    \"\"\"\n",
    "    gdf_copy = gdf.copy()\n",
    "    \n",
    "    for col in gdf_copy.columns:\n",
    "        # Convert large integers to strings, eg. \"osm_id\" col in Water shp\n",
    "        if gdf_copy[col].dtype == 'float64':\n",
    "            gdf_copy[col] = gdf_copy[col].astype(str)\n",
    "        \n",
    "        # Handle datetime columns, eg. \"date\" cols\n",
    "        if pd.api.types.is_datetime64_any_dtype(gdf_copy[col]): \n",
    "            # Convert to string in ISO format\n",
    "            gdf_copy[col] = gdf_copy[col].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return gdf_copy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_and_save_shapefile(input_gdf, dst_crs, output_path):\n",
    "    \"\"\"\n",
    "    Reproject GeoDataFrame and handle large integer fields\n",
    "    \"\"\"\n",
    "    # Create a copy of the GeoDataFrame to avoid modifying the original\n",
    "\n",
    "    gdf_preprocessed = preprocess_cols(input_gdf)\n",
    "    \n",
    "    # Reproject\n",
    "    gdf_reprojected = gdf_preprocessed.to_crs(dst_crs)\n",
    "    \n",
    "    # Save with modified field handling\n",
    "    gdf_reprojected.to_file(output_path)\n",
    "    \n",
    "    return gdf_reprojected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprojection and masking completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load shapefiles\n",
    "road = gpd.read_file(Road_path)\n",
    "water = gpd.read_file(Water_path)\n",
    "dist = gpd.read_file(Dist_path)\n",
    "\n",
    "# Open reference raster to get CRS and other parameters\n",
    "with rasterio.open(ref_path) as src_ref:\n",
    "    dst_crs = src_ref.crs\n",
    "    dst_transform = src_ref.transform\n",
    "    dst_height = src_ref.height\n",
    "    dst_width = src_ref.width\n",
    "    nodata_value = src_ref.nodata\n",
    "    \n",
    "    # Read the reference raster data\n",
    "    ref_array = src_ref.read(1)\n",
    "    \n",
    "    # Create a mask for valid data (where values are not nodata)\n",
    "    valid_mask = ref_array != nodata_value\n",
    "\n",
    "    # Reproject and save shapefiles with field handling\n",
    "    road_reprojected = reproject_and_save_shapefile(\n",
    "        road, \n",
    "        dst_crs, \n",
    "        os.path.join(reproject_path, 'road_reprojected.shp')\n",
    "    )\n",
    "    \n",
    "    water_reprojected = reproject_and_save_shapefile(\n",
    "        water, \n",
    "        dst_crs, \n",
    "        os.path.join(reproject_path, 'water_reprojected.shp')\n",
    "    )\n",
    "    \n",
    "    dist_reprojected = reproject_and_save_shapefile(\n",
    "        dist, \n",
    "        dst_crs, \n",
    "        os.path.join(reproject_path, 'dist_reprojected.shp')\n",
    "    )\n",
    "\n",
    "    # Create a new raster with the valid data mask\n",
    "    profile = src_ref.profile.copy()\n",
    "    profile.update({\n",
    "        'dtype': rasterio.float64,\n",
    "        'nodata': nodata_value,\n",
    "        'compress': 'lzw'\n",
    "    })\n",
    "\n",
    "    # Mask the reference raster\n",
    "    masked_array = np.where(valid_mask, ref_array, nodata_value).astype(rasterio.float32)\n",
    "\n",
    "    # Save the masked raster\n",
    "    masked_raster_path = os.path.join(reproject_path, '2010LCT_masked.tif')\n",
    "    with rasterio.open(masked_raster_path, 'w', **profile) as dst:\n",
    "        dst.write(masked_array, 1)\n",
    "\n",
    "print(\"Reprojection and masking completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'land': {2010: array([[-128, -128, -128, ..., -128, -128, -128],\n",
       "         [-128, -128, -128, ..., -128, -128, -128],\n",
       "         [-128, -128, -128, ..., -128, -128, -128],\n",
       "         ...,\n",
       "         [-128, -128, -128, ..., -128, -128, -128],\n",
       "         [-128, -128, -128, ..., -128, -128, -128],\n",
       "         [-128, -128, -128, ..., -128, -128, -128]], dtype=int8),\n",
       "  2011: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "  2012: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "  2013: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "  2014: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "  2015: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "  2016: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "  2017: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "  2018: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "  2019: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "  2020: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "  2021: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "  2022: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "  2023: array([[255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)},\n",
       " 'rainfall': {2010: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2011: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2012: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2013: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2014: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2015: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2016: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2017: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2018: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2019: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2020: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2021: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2022: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2023: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32)},\n",
       " 'pop': {2010: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2015: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       "  2020: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         ...,\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32)},\n",
       " 'popdens': {2010: array([[ 5.9743428e-01,  5.9699219e-01,  5.9500593e-01, ...,\n",
       "           3.6370841e-01,  3.6354756e-01,  3.5797882e-01],\n",
       "         [ 5.9688854e-01,  5.9624946e-01,  5.9757793e-01, ...,\n",
       "           3.7036008e-01,  3.6921677e-01,  3.6462474e-01],\n",
       "         [ 5.9786439e-01,  5.9678191e-01,  5.9903514e-01, ...,\n",
       "           3.7723863e-01,  3.7500769e-01,  3.7118348e-01],\n",
       "         ...,\n",
       "         [-9.9999000e+04, -9.9999000e+04, -9.9999000e+04, ...,\n",
       "          -9.9999000e+04, -9.9999000e+04, -9.9999000e+04],\n",
       "         [-9.9999000e+04, -9.9999000e+04, -9.9999000e+04, ...,\n",
       "          -9.9999000e+04, -9.9999000e+04, -9.9999000e+04],\n",
       "         [-9.9999000e+04, -9.9999000e+04, -9.9999000e+04, ...,\n",
       "          -9.9999000e+04, -9.9999000e+04, -9.9999000e+04]], dtype=float32),\n",
       "  2015: array([[ 7.7878630e-01,  7.5640380e-01,  7.3828506e-01, ...,\n",
       "           4.4821098e-01,  4.4892824e-01,  4.4111651e-01],\n",
       "         [ 7.9363716e-01,  7.7879030e-01,  7.6996678e-01, ...,\n",
       "           4.5637923e-01,  4.5468375e-01,  4.4677967e-01],\n",
       "         [ 8.0691355e-01,  7.9664248e-01,  7.9253930e-01, ...,\n",
       "           4.6356115e-01,  4.6003914e-01,  4.5320293e-01],\n",
       "         ...,\n",
       "         [-9.9999000e+04, -9.9999000e+04, -9.9999000e+04, ...,\n",
       "          -9.9999000e+04, -9.9999000e+04, -9.9999000e+04],\n",
       "         [-9.9999000e+04, -9.9999000e+04, -9.9999000e+04, ...,\n",
       "          -9.9999000e+04, -9.9999000e+04, -9.9999000e+04],\n",
       "         [-9.9999000e+04, -9.9999000e+04, -9.9999000e+04, ...,\n",
       "          -9.9999000e+04, -9.9999000e+04, -9.9999000e+04]], dtype=float32),\n",
       "  2020: array([[ 9.4151551e-01,  9.0794706e-01,  8.9484161e-01, ...,\n",
       "           5.6172949e-01,  5.6941324e-01,  5.6299049e-01],\n",
       "         [ 9.5381898e-01,  9.1379398e-01,  9.0574944e-01, ...,\n",
       "           5.7647359e-01,  5.8064806e-01,  5.7570869e-01],\n",
       "         [ 9.7771901e-01,  9.3166119e-01,  9.2073399e-01, ...,\n",
       "           5.8804291e-01,  5.9070361e-01,  5.8848339e-01],\n",
       "         ...,\n",
       "         [-9.9999000e+04, -9.9999000e+04, -9.9999000e+04, ...,\n",
       "          -9.9999000e+04, -9.9999000e+04, -9.9999000e+04],\n",
       "         [-9.9999000e+04, -9.9999000e+04, -9.9999000e+04, ...,\n",
       "          -9.9999000e+04, -9.9999000e+04, -9.9999000e+04],\n",
       "         [-9.9999000e+04, -9.9999000e+04, -9.9999000e+04, ...,\n",
       "          -9.9999000e+04, -9.9999000e+04, -9.9999000e+04]], dtype=float32)},\n",
       " 'gpp': {2010: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2011: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2012: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2013: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2014: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2015: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2016: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2017: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2018: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2019: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2020: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2021: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2022: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32),\n",
       "  2023: array([[65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         ...,\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.],\n",
       "         [65535., 65535., 65535., ..., 65535., 65535., 65535.]],\n",
       "        dtype=float32)}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overview of all data\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['land', 'rainfall', 'pop', 'popdens', 'gpp'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invalid data in each dict:\n",
    "# 1. negative value:\n",
    "#   -128.0 in land\n",
    "#   -3.4028235e+38 in rainfall and pop\n",
    "# 2. positive value:\n",
    "#   65535 in GPP\n",
    "\n",
    "def create_mask(data_dict, invalid_criteria=None):\n",
    "    \"\"\"\n",
    "    Create masks for invalid data in a nested dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dict : dict\n",
    "        Nested dictionary with data types as top-level keys\n",
    "        and years as second-level keys\n",
    "    invalid_criteria : dict, optional\n",
    "        Specific invalid data criteria for each data type\n",
    "        Default: {'default': {'condition': lambda x: (x < 0) & (x != 0)}}\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Nested dictionary of masks with data types and years as keys\n",
    "    \"\"\"\n",
    "    # Default invalid data criteria\n",
    "    default_criteria = {\n",
    "        'default': {'condition': lambda x: (x < 0)}\n",
    "    }\n",
    "    \n",
    "    # Merge default criteria with provided criteria\n",
    "    if invalid_criteria is None:\n",
    "        invalid_criteria = default_criteria\n",
    "    else:\n",
    "        for key, value in default_criteria.items():\n",
    "            if key not in invalid_criteria:\n",
    "                invalid_criteria[key] = value\n",
    "    \n",
    "    # Create mask dictionary\n",
    "    mask_dict = {}\n",
    "    \n",
    "    # Iterate through data types\n",
    "    for data_type, year_data in data_dict.items():\n",
    "        mask_dict[data_type] = {}\n",
    "        \n",
    "        # Determine criteria for this data type\n",
    "        criteria = invalid_criteria.get(data_type, invalid_criteria['default'])\n",
    "        condition = criteria['condition']\n",
    "        \n",
    "        # Create masks for each year\n",
    "        for year, array in year_data.items():\n",
    "            # Apply the condition to create a boolean mask\n",
    "            mask_dict[data_type][year] = condition(array)\n",
    "    \n",
    "    return mask_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(data_dict, mask_dict):\n",
    "    \"\"\"\n",
    "    Apply masks to data dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dict : dict\n",
    "        Nested dictionary of data arrays\n",
    "    mask_dict : dict\n",
    "        Nested dictionary of masks\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Nested dictionary of masked arrays\n",
    "    \"\"\"\n",
    "    masked_data_dict = {}\n",
    "    \n",
    "    # Iterate through data types\n",
    "    for data_type, year_data in data_dict.items():\n",
    "        masked_data_dict[data_type] = {}\n",
    "        \n",
    "        # Apply mask for each year\n",
    "        for year, array in year_data.items():\n",
    "            masked_data_dict[data_type][year] = np.ma.array(\n",
    "                array, \n",
    "                mask=mask_dict[data_type][year]\n",
    "            )\n",
    "    \n",
    "    return masked_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_df(masked_data_dict):\n",
    "    \"\"\"\n",
    "    Convert masked data dictionary to a comprehensive DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    masked_data_dict : dict\n",
    "        Nested dictionary of masked arrays\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Comprehensive DataFrame with data information\n",
    "    \"\"\"\n",
    "    data_rows = []\n",
    "    \n",
    "    # Iterate through data types\n",
    "    for data_type, year_data in masked_data_dict.items():\n",
    "        # Iterate through years\n",
    "        for year, masked_array in year_data.items():\n",
    "            # Flatten the data while preserving the mask\n",
    "            flattened_data = masked_array.compressed()\n",
    "            \n",
    "            # Create rows for each valid data point\n",
    "            for value in flattened_data:\n",
    "                data_rows.append({\n",
    "                    'data_type': data_type,\n",
    "                    'year': year,\n",
    "                    'value': value\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_rows)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_masked_data(masked_data_dict):\n",
    "    \"\"\"\n",
    "    Provide summary statistics of masked data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    masked_data_dict : dict\n",
    "        Nested dictionary of masked arrays\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Summary statistics for each data type and year\n",
    "    \"\"\"\n",
    "    summary_stats = {}\n",
    "    \n",
    "    # Iterate through data types\n",
    "    for data_type, year_data in masked_data_dict.items():\n",
    "        summary_stats[data_type] = {}\n",
    "        \n",
    "        # Compute statistics for each year\n",
    "        for year, masked_array in year_data.items():\n",
    "            # Compute statistics on valid (unmasked) data\n",
    "            valid_data = masked_array.compressed()\n",
    "            \n",
    "            summary_stats[data_type][year] = {\n",
    "                'total_points': masked_array.size,\n",
    "                'valid_points': len(valid_data),\n",
    "                'masked_points': masked_array.size - len(valid_data),\n",
    "                'masked_percentage': (masked_array.size - len(valid_data)) / masked_array.size * 100,\n",
    "                'min': np.min(valid_data) if len(valid_data) > 0 else None,\n",
    "                'max': np.max(valid_data) if len(valid_data) > 0 else None,\n",
    "                'mean': np.mean(valid_data) if len(valid_data) > 0 else None,\n",
    "                'median': np.median(valid_data) if len(valid_data) > 0 else None\n",
    "            }\n",
    "    \n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_dict, invalid_criteria=None):\n",
    "    \"\"\"\n",
    "    Main processing function for data masking.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dict : dict\n",
    "        Nested dictionary of data arrays\n",
    "    invalid_criteria : dict, optional\n",
    "        Custom invalid data criteria\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Processed data results\n",
    "    \"\"\"\n",
    "    # Create masks\n",
    "    mask_dict = create_mask(data_dict, invalid_criteria)\n",
    "    \n",
    "    # Apply masks\n",
    "    masked_data_dict = apply_mask(data_dict, mask_dict)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = dict_to_df(masked_data_dict)\n",
    "    \n",
    "    # Get summary statistics\n",
    "    summary = analyze_masked_data(masked_data_dict)\n",
    "    \n",
    "    return {\n",
    "        'masked_data': masked_data_dict,\n",
    "        'dataframe': df,\n",
    "        'summary': summary,\n",
    "        'mask_dict': mask_dict\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested Dictionary Data Masking Module\n",
      "Supports NumPy array-compatible masking for different data types\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"Nested Dictionary Data Masking Module\")\n",
    "    print(\"Supports NumPy array-compatible masking for different data types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom criteria example\n",
    "\n",
    "custom_criteria = {\n",
    "    'land': {'condition': lambda x: np.logical_or(x < 0, x == 255)},\n",
    "    'gpp': {'condition': lambda x: x > 65000},\n",
    "    'pop': {'condition': lambda x: x < 0}\n",
    "}\n",
    "\n",
    "# Process data\n",
    "prepared_df = main(all_data, invalid_criteria=custom_criteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['masked_data', 'dataframe', 'summary', 'mask_dict'])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking Summary for Land\n",
      " {'total_points': 434485, 'valid_points': 168212, 'masked_points': 266273, 'masked_percentage': 61.28473940412212, 'min': 7, 'max': 16, 'mean': 11.579542482105914, 'median': 10.0}\n",
      "Masking Summary for GPP\n",
      " {'total_points': 434485, 'valid_points': 108061, 'masked_points': 326424, 'masked_percentage': 75.12894576337503, 'min': 233.0, 'max': 3720.0, 'mean': 1159.0868, 'median': 1087.0}\n",
      "Masking Summary for POP\n",
      " {'total_points': 434485, 'valid_points': 167202, 'masked_points': 267283, 'masked_percentage': 61.51719852238857, 'min': 0.12940584, 'max': 1664.9595, 'mean': 10.732144, 'median': 4.919127}\n",
      "Masking Summary for Pop Dens\n",
      " {'total_points': 434485, 'valid_points': 409430, 'masked_points': 25055, 'masked_percentage': 5.76659723580791, 'min': 0.05303842, 'max': 1713.4237, 'mean': 11.089264, 'median': 4.8886147}\n",
      "Masking Summary for Rainfall\n",
      " {'total_points': 434485, 'valid_points': 167811, 'masked_points': 266674, 'masked_percentage': 61.37703257880018, 'min': 52.972946, 'max': 511.3252, 'mean': 249.91487, 'median': 242.17699}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Masking Summary for Land\\n {prepared_df['summary']['land'][2023]}\")\n",
    "print(f\"Masking Summary for GPP\\n {prepared_df['summary']['gpp'][2023]}\")\n",
    "print(f\"Masking Summary for POP\\n {prepared_df['summary']['pop'][2020]}\")\n",
    "print(f\"Masking Summary for Pop Dens\\n {prepared_df['summary']['popdens'][2020]}\")\n",
    "print(f\"Masking Summary for Rainfall\\n {prepared_df['summary']['rainfall'][2023]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2010,2015,2020 population per pixel\n",
    "# 1. <0 invalid data ->0\n",
    "# 2. LAT LON df\n",
    "# 3. calculation\n",
    "all_data['gridded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridded land use -- get the hotspots top3 pixel\n",
    "# Store transformed dataframes\n",
    "df_dict = {}\n",
    "\n",
    "# Process each year's array\n",
    "for year, array in all_data['gridded'].items():\n",
    "    # Replace negative values with 0\n",
    "    array = np.maximum(array, 0)\n",
    "\n",
    "    # Get row (lat) and column (lon) indices\n",
    "    rows, cols = np.indices(array.shape)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'lat': rows.flatten(),  # Row index as latitude\n",
    "        'lon': cols.flatten(),  # Column index as longitude\n",
    "        'value': array.flatten()  # Flattened values\n",
    "    })\n",
    "\n",
    "    # Store in dictionary\n",
    "    df_dict[year] = df\n",
    "    \n",
    "    # create population dataframe for finalised merged df\n",
    "    population_df_list = []\n",
    "    for year, df in df_dict.items():\n",
    "        # Add a 'year' column to each DataFrame\n",
    "        df['year'] = year\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        population_df_list.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    population_df = pd.concat(population_df_list, ignore_index=True)\n",
    "    \n",
    "    population_df.to_csv('population_df.csv')\n",
    "\n",
    "# Compute differences\n",
    "df_diff_2015_2010 = df_dict[2015].copy()\n",
    "df_diff_2020_2015 = df_dict[2020].copy()\n",
    "\n",
    "# Subtract values\n",
    "df_diff_2015_2010['value'] -= df_dict[2010]['value']\n",
    "df_diff_2020_2015['value'] -= df_dict[2015]['value']\n",
    "\n",
    "# rename value column to avoid confusion\n",
    "df_diff_2015_2010 = df_diff_2015_2010.rename(columns={'value': 'diff'})\n",
    "df_diff_2020_2015 = df_diff_2020_2015.rename(columns={'value': 'diff'})\n",
    "\n",
    "# Sort df_diff_20xx_20xx in descending order\n",
    "df_diff_2015_2010_sorted = df_diff_2015_2010.sort_values(by='diff', ascending=False)\n",
    "df_diff_2020_2015_sorted = df_diff_2020_2015.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Extract the top 3 largest and bottom 3 smallest values\n",
    "top_3_2015_2010 = df_diff_2015_2010_sorted.head(3)\n",
    "bottom_3_2015_2010 = df_diff_2015_2010_sorted.tail(3)\n",
    "selected_2015_2010 = pd.concat([top_3_2015_2010, bottom_3_2015_2010], ignore_index=True)\n",
    "selected_2015_2010['year'] = 2015\n",
    "selected_2015_2010['type'] = 'population'\n",
    "\n",
    "# Extract the top 3 largest and bottom 3 smallest values\n",
    "top_3_2020_2015 = df_diff_2020_2015_sorted.head(3)\n",
    "bottom_3_2020_2015 = df_diff_2020_2015_sorted.tail(3)\n",
    "selected_2020_2015 = pd.concat([top_3_2020_2015, bottom_3_2020_2015], ignore_index=True)\n",
    "selected_2020_2015['year'] = 2020\n",
    "selected_2020_2015['type'] = 'population'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPP extract the top 3 largest and bottem 3 smallest data point\n",
    "gross_df = pd.read_csv('merged_df.csv')\n",
    "\n",
    "# Extract data for each year\n",
    "df_2010 = gross_df[gross_df['year'] == 2010][['lat', 'lon', 'gross']]\n",
    "df_2015 = gross_df[gross_df['year'] == 2015][['lat', 'lon', 'gross']]\n",
    "df_2020 = gross_df[gross_df['year'] == 2020][['lat', 'lon', 'gross']]\n",
    "\n",
    "# Merge on lat and lon\n",
    "gross_df_2015_2010 = df_2015.merge(df_2010, on=['lat', 'lon'], suffixes=('_2015', '_2010'))\n",
    "gross_df_2020_2015 = df_2020.merge(df_2015, on=['lat', 'lon'], suffixes=('_2020', '_2015'))\n",
    "\n",
    "\n",
    "# Compute difference\n",
    "gross_df_2015_2010['diff'] = gross_df_2015_2010['gross_2015'] - gross_df_2015_2010['gross_2010']\n",
    "gross_df_2020_2015['diff'] = gross_df_2020_2015['gross_2020'] - gross_df_2020_2015['gross_2015']\n",
    "\n",
    "\n",
    "# Keep only relevant columns\n",
    "gross_df_2020_2015 = gross_df_2020_2015[['lat', 'lon', 'diff']]\n",
    "gross_df_2015_2010 = gross_df_2015_2010[['lat', 'lon', 'diff']]\n",
    "\n",
    "# Sort by 'value' in descending order\n",
    "gross_df_diff_2015_2010_sorted = gross_df_2015_2010.sort_values(by='diff', ascending=False)\n",
    "gross_df_diff_2020_2015_sorted = gross_df_2020_2015.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Extract the top 3 largest and bottom 3 smallest values\n",
    "gross_top_3_2015_2010 = gross_df_diff_2015_2010_sorted.head(3)\n",
    "gross_bottom_3_2015_2010 = gross_df_diff_2015_2010_sorted.tail(3)\n",
    "gross_select_2015_2010 = pd.concat([gross_top_3_2015_2010, gross_bottom_3_2015_2010], ignore_index=True)\n",
    "gross_select_2015_2010['year'] = 2015\n",
    "gross_select_2015_2010['type'] = 'gpp'\n",
    "\n",
    "gross_top_3_2020_2015 = gross_df_diff_2020_2015_sorted.head(3)\n",
    "gross_bottom_3_2020_2015 = gross_df_diff_2020_2015_sorted.tail(3)\n",
    "gross_select_2020_2015 = pd.concat([gross_top_3_2020_2015, gross_bottom_3_2020_2015], ignore_index=True)\n",
    "gross_select_2020_2015['year'] = 2020\n",
    "gross_select_2020_2015['type'] = 'gpp'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the dataframes\n",
    "dfs = [selected_2015_2010, selected_2020_2015, gross_select_2015_2010, gross_select_2020_2015]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "file_path = os.path.join(save_path, 'combined_population_gpp_pixcel.csv')\n",
    "\n",
    "combined_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrrr = all_data['gridded'][2010]\n",
    "filtered_arr = arrrr[arrrr >= 0]\n",
    "len(filtered_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
