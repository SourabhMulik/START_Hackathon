{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reprojection\n",
    "\n",
    "**CRS of Land as reprojection reference.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproject_path = 'Datasets_Hackathon/Reprojected_Data'\n",
    "csv_path = 'For_dashboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for year 2010...\n",
      "Successfully reprojected and saved Datasets_Hackathon/reprojected_data/gridded_reprojected_2010.tif\n",
      "Processing data for year 2011...\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2011.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2011.tif: No such file or directory\n",
      "Processing data for year 2012...\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2012.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2012.tif: No such file or directory\n",
      "Processing data for year 2013...\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2013.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2013.tif: No such file or directory\n",
      "Processing data for year 2014...\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2014.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2014.tif: No such file or directory\n",
      "Processing data for year 2015...\n",
      "Successfully reprojected and saved Datasets_Hackathon/reprojected_data/gridded_reprojected_2015.tif\n",
      "Processing data for year 2016...\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2016.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2016.tif: No such file or directory\n",
      "Processing data for year 2017...\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2017.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2017.tif: No such file or directory\n",
      "Processing data for year 2018...\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2018.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2018.tif: No such file or directory\n",
      "Processing data for year 2019...\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2019.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2019.tif: No such file or directory\n",
      "Processing data for year 2020...\n",
      "Successfully reprojected and saved Datasets_Hackathon/reprojected_data/gridded_reprojected_2020.tif\n",
      "Processing data for year 2021...\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2021.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2021.tif: No such file or directory\n",
      "Processing data for year 2022...\n",
      "Error processing Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2022.tif: Datasets_Hackathon/Gridded_Population_Density_Data/Assaba_Pop_2022.tif: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Define years list\n",
    "years = range(2010, 2023)\n",
    "\n",
    "# Define input and output paths\n",
    "data_path = 'Datasets_Hackathon'\n",
    "\n",
    "# List of data categories with their appropriate resampling methods and filename formats\n",
    "datasets = [\n",
    "    {'name': 'Land_Cover_Data', 'file_format': '{year}LCT.tif', 'is_reference': True},\n",
    "    {'name': 'Gridded_Population_Density_Data', 'file_format': 'Assaba_Pop_{year}.tif','resampling': Resampling.bilinear},\n",
    "]\n",
    "\n",
    "# Dictionary to store data for all years and all datasets\n",
    "all_data = {}\n",
    "for dataset in datasets:\n",
    "    short_name = dataset['name'].split('_')[0].lower()\n",
    "    all_data[short_name] = {}\n",
    "\n",
    "# Loop through each year\n",
    "for year in years:\n",
    "    print(f\"Processing data for year {year}...\")\n",
    "    \n",
    "    # First, open the reference dataset (GPP)\n",
    "    ref_dataset = next(d for d in datasets if d['is_reference'])\n",
    "    ref_file = os.path.join(data_path, ref_dataset['name'], ref_dataset['file_format'].format(year=year))\n",
    "    \n",
    "    try:\n",
    "        with rasterio.open(ref_file) as src_ref:\n",
    "            # Get reference metadata\n",
    "            dst_crs = src_ref.crs\n",
    "            dst_transform = src_ref.transform\n",
    "            dst_height = src_ref.height\n",
    "            dst_width = src_ref.width\n",
    "            \n",
    "            # Read reference data (land)\n",
    "            land_data = src_ref.read(1)\n",
    "            all_data['land'][year] = land_data\n",
    "            \n",
    "            # Store reference profile for output files\n",
    "            profile = src_ref.profile.copy()\n",
    "            profile.update(dtype=rasterio.float32, count=1)\n",
    "            \n",
    "            # Process each non-reference dataset\n",
    "            for dataset in [d for d in datasets if not d.get('is_reference', False)]:\n",
    "                dataset_name = dataset['name'].split('_')[0].lower()  # Extract short name\n",
    "                \n",
    "                # Construct input filename using the file format template\n",
    "                input_file = os.path.join(data_path, dataset['name'], dataset['file_format'].format(year=year))\n",
    "                output_file = os.path.join(reproject_path, f\"{dataset_name}_reprojected_{year}.tif\")\n",
    "                \n",
    "                # Create destination array\n",
    "                dst_array = np.zeros((dst_height, dst_width), dtype=rasterio.float32)\n",
    "                \n",
    "                # Open and reproject\n",
    "                try:\n",
    "                    with rasterio.open(input_file) as src:\n",
    "                        reproject(\n",
    "                            source=rasterio.band(src, 1),\n",
    "                            destination=dst_array,\n",
    "                            src_transform=src.transform,\n",
    "                            src_crs=src.crs,\n",
    "                            dst_transform=dst_transform,\n",
    "                            dst_crs=dst_crs,\n",
    "                            resampling=dataset['resampling']\n",
    "                        )\n",
    "                        \n",
    "                        # Store in all_data dictionary by year\n",
    "                        all_data[dataset_name][year] = dst_array\n",
    "                        \n",
    "                        # Save reprojected data\n",
    "                        with rasterio.open(output_file, 'w', **profile) as dst:\n",
    "                            dst.write(dst_array, 1)\n",
    "                            \n",
    "                        print(f\"Successfully reprojected and saved {output_file}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {input_file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing year {year}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2010: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        ...,\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       " 2015: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        ...,\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32),\n",
       " 2020: array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        ...,\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "        [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n",
       "         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2010,2015,2020 population per pixel\n",
    "# 1. <0 invalid data ->0\n",
    "# 2. LAT LON df\n",
    "# 3. calculation\n",
    "all_data['gridded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridded land use -- get the hotspots top3 pixel\n",
    "# Store transformed dataframes\n",
    "df_dict = {}\n",
    "\n",
    "# Process each year's array\n",
    "for year, array in all_data['gridded'].items():\n",
    "    # Replace negative values with 0\n",
    "    array = np.maximum(array, 0)\n",
    "\n",
    "    # Get row (lat) and column (lon) indices\n",
    "    rows, cols = np.indices(array.shape)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'lat': rows.flatten(),  # Row index as latitude\n",
    "        'lon': cols.flatten(),  # Column index as longitude\n",
    "        'value': array.flatten()  # Flattened values\n",
    "    })\n",
    "\n",
    "    # Store in dictionary\n",
    "    df_dict[year] = df\n",
    "    \n",
    "    # create population dataframe for finalised merged df\n",
    "    population_df_list = []\n",
    "    for year, df in df_dict.items():\n",
    "        # Add a 'year' column to each DataFrame\n",
    "        df['year'] = year\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        population_df_list.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    population_df = pd.concat(population_df_list, ignore_index=True)\n",
    "    \n",
    "    population_df.to_csv('population_df.csv')\n",
    "\n",
    "# Compute differences\n",
    "df_diff_2015_2010 = df_dict[2015].copy()\n",
    "df_diff_2020_2015 = df_dict[2020].copy()\n",
    "\n",
    "# Subtract values\n",
    "df_diff_2015_2010['value'] -= df_dict[2010]['value']\n",
    "df_diff_2020_2015['value'] -= df_dict[2015]['value']\n",
    "\n",
    "# rename value column to avoid confusion\n",
    "df_diff_2015_2010 = df_diff_2015_2010.rename(columns={'value': 'diff'})\n",
    "df_diff_2020_2015 = df_diff_2020_2015.rename(columns={'value': 'diff'})\n",
    "\n",
    "# Sort df_diff_20xx_20xx in descending order\n",
    "df_diff_2015_2010_sorted = df_diff_2015_2010.sort_values(by='diff', ascending=False)\n",
    "df_diff_2020_2015_sorted = df_diff_2020_2015.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Extract the top 3 largest and bottom 3 smallest values\n",
    "top_3_2015_2010 = df_diff_2015_2010_sorted.head(3)\n",
    "bottom_3_2015_2010 = df_diff_2015_2010_sorted.tail(3)\n",
    "selected_2015_2010 = pd.concat([top_3_2015_2010, bottom_3_2015_2010], ignore_index=True)\n",
    "selected_2015_2010['year'] = 2015\n",
    "selected_2015_2010['type'] = 'population'\n",
    "\n",
    "# Extract the top 3 largest and bottom 3 smallest values\n",
    "top_3_2020_2015 = df_diff_2020_2015_sorted.head(3)\n",
    "bottom_3_2020_2015 = df_diff_2020_2015_sorted.tail(3)\n",
    "selected_2020_2015 = pd.concat([top_3_2020_2015, bottom_3_2020_2015], ignore_index=True)\n",
    "selected_2020_2015['year'] = 2020\n",
    "selected_2020_2015['type'] = 'population'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPP extract the top 3 largest and bottem 3 smallest data point\n",
    "gross_df = pd.read_csv('merged_df.csv')\n",
    "\n",
    "# Extract data for each year\n",
    "df_2010 = gross_df[gross_df['year'] == 2010][['lat', 'lon', 'gross']]\n",
    "df_2015 = gross_df[gross_df['year'] == 2015][['lat', 'lon', 'gross']]\n",
    "df_2020 = gross_df[gross_df['year'] == 2020][['lat', 'lon', 'gross']]\n",
    "\n",
    "# Merge on lat and lon\n",
    "gross_df_2015_2010 = df_2015.merge(df_2010, on=['lat', 'lon'], suffixes=('_2015', '_2010'))\n",
    "gross_df_2020_2015 = df_2020.merge(df_2015, on=['lat', 'lon'], suffixes=('_2020', '_2015'))\n",
    "\n",
    "\n",
    "# Compute difference\n",
    "gross_df_2015_2010['diff'] = gross_df_2015_2010['gross_2015'] - gross_df_2015_2010['gross_2010']\n",
    "gross_df_2020_2015['diff'] = gross_df_2020_2015['gross_2020'] - gross_df_2020_2015['gross_2015']\n",
    "\n",
    "\n",
    "# Keep only relevant columns\n",
    "gross_df_2020_2015 = gross_df_2020_2015[['lat', 'lon', 'diff']]\n",
    "gross_df_2015_2010 = gross_df_2015_2010[['lat', 'lon', 'diff']]\n",
    "\n",
    "# Sort by 'value' in descending order\n",
    "gross_df_diff_2015_2010_sorted = gross_df_2015_2010.sort_values(by='diff', ascending=False)\n",
    "gross_df_diff_2020_2015_sorted = gross_df_2020_2015.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Extract the top 3 largest and bottom 3 smallest values\n",
    "gross_top_3_2015_2010 = gross_df_diff_2015_2010_sorted.head(3)\n",
    "gross_bottom_3_2015_2010 = gross_df_diff_2015_2010_sorted.tail(3)\n",
    "gross_select_2015_2010 = pd.concat([gross_top_3_2015_2010, gross_bottom_3_2015_2010], ignore_index=True)\n",
    "gross_select_2015_2010['year'] = 2015\n",
    "gross_select_2015_2010['type'] = 'gpp'\n",
    "\n",
    "gross_top_3_2020_2015 = gross_df_diff_2020_2015_sorted.head(3)\n",
    "gross_bottom_3_2020_2015 = gross_df_diff_2020_2015_sorted.tail(3)\n",
    "gross_select_2020_2015 = pd.concat([gross_top_3_2020_2015, gross_bottom_3_2020_2015], ignore_index=True)\n",
    "gross_select_2020_2015['year'] = 2020\n",
    "gross_select_2020_2015['type'] = 'gpp'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the dataframes\n",
    "dfs = [selected_2015_2010, selected_2020_2015, gross_select_2015_2010, gross_select_2020_2015]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "file_path = os.path.join(save_path, 'combined_population_gpp_pixcel.csv')\n",
    "\n",
    "combined_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167202"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arrrr = all_data['gridded'][2010]\n",
    "filtered_arr = arrrr[arrrr >= 0]\n",
    "len(filtered_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
